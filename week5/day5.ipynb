{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfe37963-1af6-44fc-a841-8e462443f5e6",
   "metadata": {},
   "source": [
    "## Trabajador experto en conocimiento\n",
    "\n",
    "### Un agente que responde preguntas y es un trabajador experto en conocimiento\n",
    "### Para ser utilizado por empleados de Insurellm, una empresa de tecnología de seguros\n",
    "### El agente debe ser preciso y la solución debe ser de bajo costo.\n",
    "\n",
    "Este proyecto utilizará RAG (Retrieval Augmented Generation, generación aumentada de recuperación) para garantizar que nuestro asistente de preguntas y respuestas tenga una alta precisión.\n",
    "\n",
    "Esta primera implementación utilizará un tipo de RAG simple, de fuerza bruta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ba2779af-84ef-4227-9e9e-6eaf0df87e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "import glob\n",
    "from dotenv import load_dotenv\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "802137aa-8a74-45e0-a487-d1974927d7ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib is building the font cache; this may take a moment.\n"
     ]
    }
   ],
   "source": [
    "# imports de langchain, plotly y Chroma\n",
    "\n",
    "from langchain.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_chroma import Chroma\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58c85082-e417-4708-9efe-81a5d55d1424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# El precio es un factor para nuestra empresa, por eso vamos a utilizar un modelo de bajo costo.\n",
    "\n",
    "MODEL = \"gpt-4o-mini\"\n",
    "db_name = \"vector_db\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee78efcb-60fe-449e-a944-40bab26261af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar variables de entorno en un archivo llamado .env\n",
    "\n",
    "load_dotenv()\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY', 'your-key-if-not-using-env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "730711a9-6ffe-4eee-8f48-d6cfb7314905",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 1252, which is longer than the specified 1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of chunks: 137\n",
      "Document types found: {'company', 'contracts', 'products', 'employees'}\n"
     ]
    }
   ],
   "source": [
    "# Leer documentos usando los cargadores de LangChain\n",
    "# Tomar todo lo que está en todas las subcarpetas de nuestra base de conocimiento\n",
    "\n",
    "folders = glob.glob(\"knowledge-base/*\")\n",
    "\n",
    "def add_metadata(doc, doc_type):\n",
    "    doc.metadata[\"doc_type\"] = doc_type\n",
    "    return doc\n",
    "\n",
    "# Con agradecimientos a CG y Jon R, estudiantes del curso, por esta solución necesaria para algunos usuarios.\n",
    "text_loader_kwargs = {'encoding': 'utf-8'}\n",
    "# Si eso no funciona, algunos usuarios de Windows podrían necesitar descomentar la siguiente línea.\n",
    "# text_loader_kwargs={'autodetect_encoding': True}\n",
    "\n",
    "documents = []\n",
    "for folder in folders:\n",
    "    doc_type = os.path.basename(folder)\n",
    "    loader = DirectoryLoader(folder, glob=\"**/*.md\", loader_cls=TextLoader, loader_kwargs=text_loader_kwargs)\n",
    "    folder_docs = loader.load()\n",
    "    documents.extend([add_metadata(doc, doc_type) for doc in folder_docs])\n",
    "\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "print(f\"Total number of chunks: {len(chunks)}\")\n",
    "print(f\"Document types found: {set(doc.metadata['doc_type'] for doc in documents)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f7d2a6-ccfa-425b-a1c3-5e55b23bd013",
   "metadata": {},
   "source": [
    "## Una nota al margen sobre las incrustaciones y los \"LLM de codificación automática\"\n",
    "\n",
    "Asignaremos cada fragmento de texto a un vector que representa el significado del texto, conocido como incrustación.\n",
    "\n",
    "OpenAI ofrece un modelo para hacer esto, que utilizaremos llamando a su API con un código LangChain.\n",
    "\n",
    "Este modelo es un ejemplo de un \"LLM de codificación automática\" que genera una salida dada una entrada completa.\n",
    "Es diferente a todos los demás LLM que hemos analizado hoy, que se conocen como \"LLM autorregresivos\", y generan tokens futuros basados ​​solo en el contexto pasado.\n",
    "\n",
    "Otro ejemplo de un LLM de codificación automática es BERT de Google. Además de la incrustación, los LLM de codificación automática se utilizan a menudo para la clasificación.\n",
    "\n",
    "### Nota al margen\n",
    "\n",
    "En la semana 8 volveremos a RAG y a las incrustaciones vectoriales, y utilizaremos un codificador vectorial de código abierto para que los datos nunca abandonen nuestra computadora; esa es una consideración importante cuando se crean sistemas empresariales y los datos deben permanecer internos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78998399-ac17-4e28-b15f-0b5f51e6ee23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorstore created with 137 documents\n"
     ]
    }
   ],
   "source": [
    "# Coloque los fragmentos de datos en un almacén de vectores que asocie una incrustación de vectores con cada fragmento\n",
    "# Chroma es una popular base de datos de vectores de código abierto basada en SQLLite\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# Eliminar si ya existe\n",
    "\n",
    "if os.path.exists(db_name):\n",
    "    Chroma(persist_directory=db_name, embedding_function=embeddings).delete_collection()\n",
    "\n",
    "# Crear almacén de vectores\n",
    "\n",
    "vectorstore = Chroma.from_documents(documents=chunks, embedding=embeddings, persist_directory=db_name)\n",
    "print(f\"Vectorstore created with {vectorstore._collection.count()} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ff2e7687-60d4-4920-a1d7-a34b9f70a250",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hay 137 vectores con 1,536 dimensiones en el almacén de vectores\n"
     ]
    }
   ],
   "source": [
    "# Investiguemos los vectores\n",
    "\n",
    "collection = vectorstore._collection\n",
    "count = collection.count()\n",
    "\n",
    "sample_embedding = collection.get(limit=1, include=[\"embeddings\"])[\"embeddings\"][0]\n",
    "dimensions = len(sample_embedding)\n",
    "print(f\"Hay {count:,} vectores con {dimensions:,} dimensiones en el almacén de vectores\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d45462-a818-441c-b010-b85b32bcf618",
   "metadata": {},
   "source": [
    "## Visualización del almacén de vectores\n",
    "\n",
    "Tomémonos un minuto para observar los documentos y sus vectores de incrustación para ver qué está sucediendo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b98adf5e-d464-4bd2-9bdf-bc5b6770263b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trabajo previo (¡con agradecimiento a Jon R por identificar y corregir un error en esto!)\n",
    "\n",
    "result = collection.get(include=['embeddings', 'documents', 'metadatas'])\n",
    "vectors = np.array(result['embeddings'])\n",
    "documents = result['documents']\n",
    "metadatas = result['metadatas']\n",
    "doc_types = [metadata['doc_type'] for metadata in metadatas]\n",
    "colors = [['blue', 'green', 'red', 'orange'][['products', 'employees', 'contracts', 'company'].index(t)] for t in doc_types]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427149d5-e5d8-4abd-bb6f-7ef0333cca21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ¡A los humanos nos resulta más fácil visualizar cosas en 2D!\n",
    "# Reducir la dimensionalidad de los vectores a 2D usando t-SNE\n",
    "# (incrustación de vecinos estocásticos distribuidos en t)\n",
    "\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "reduced_vectors = tsne.fit_transform(vectors)\n",
    "\n",
    "# Create the 2D scatter plot\n",
    "fig = go.Figure(data=[go.Scatter(\n",
    "    x=reduced_vectors[:, 0],\n",
    "    y=reduced_vectors[:, 1],\n",
    "    mode='markers',\n",
    "    marker=dict(size=5, color=colors, opacity=0.8),\n",
    "    text=[f\"Type: {t}<br>Text: {d[:100]}...\" for t, d in zip(doc_types, documents)],\n",
    "    hoverinfo='text'\n",
    ")])\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Visualización 2D Chroma Vector Store',\n",
    "    scene=dict(xaxis_title='x',yaxis_title='y'),\n",
    "    width=800,\n",
    "    height=600,\n",
    "    margin=dict(r=20, b=10, l=10, t=40)\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1418e88-acd5-460a-bf2b-4e6efc88e3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try 3D!\n",
    "\n",
    "tsne = TSNE(n_components=3, random_state=42)\n",
    "reduced_vectors = tsne.fit_transform(vectors)\n",
    "\n",
    "# Create the 3D scatter plot\n",
    "fig = go.Figure(data=[go.Scatter3d(\n",
    "    x=reduced_vectors[:, 0],\n",
    "    y=reduced_vectors[:, 1],\n",
    "    z=reduced_vectors[:, 2],\n",
    "    mode='markers',\n",
    "    marker=dict(size=5, color=colors, opacity=0.8),\n",
    "    text=[f\"Type: {t}<br>Text: {d[:100]}...\" for t, d in zip(doc_types, documents)],\n",
    "    hoverinfo='text'\n",
    ")])\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Visualización 3D Chroma Vector Store',\n",
    "    scene=dict(xaxis_title='x', yaxis_title='y', zaxis_title='z'),\n",
    "    width=900,\n",
    "    height=700,\n",
    "    margin=dict(r=20, b=10, l=10, t=40)\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9468860b-86a2-41df-af01-b2400cc985be",
   "metadata": {},
   "source": [
    "## Es hora de usar LangChain para unirlo todo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "129c7d1e-0094-4479-9459-f9360b95f244",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new Chat with OpenAI\n",
    "llm = ChatOpenAI(temperature=0.7, model_name=MODEL)\n",
    "\n",
    "# set up the conversation memory for the chat\n",
    "memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n",
    "\n",
    "# the retriever is an abstraction over the VectorStore that will be used during RAG\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# juntando todo: configure la cadena de conversación con GPT 3.5 LLM, el almacén vectorial y la memoria\n",
    "conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "968e7bf2-e862-4679-a11f-6c1efb6ec8ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Insurellm es una innovadora empresa de tecnología de seguros fundada en 2015 que ofrece soluciones de software para el sector de seguros, incluyendo productos como Carllm, Homellm, Rellm y Marketllm. Con 200 empleados y más de 300 clientes en todo el mundo, Insurellm busca revolucionar la industria de seguros a través de productos innovadores.\n"
     ]
    }
   ],
   "source": [
    "# Vamos a intentar una pregunta sencilla\n",
    "\n",
    "query = \"Por favor, explique qué es Insurellm en un par de frases.\"\n",
    "result = conversation_chain.invoke({\"question\": query})\n",
    "print(result[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5b5a9013-d5d4-4e25-9e7c-cdbb4f33e319",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up a new conversation memory for the chat\n",
    "memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n",
    "\n",
    "# putting it together: set up the conversation chain with the GPT 4o-mini LLM, the vector store and memory\n",
    "conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbbcb659-13ce-47ab-8a5e-01b930494964",
   "metadata": {},
   "source": [
    "## Ahora, lo mostraremos en Gradio usando la interfaz de Chat:\n",
    "\n",
    "Una forma rápida y sencilla de crear un prototipo de chat con un LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c3536590-85c7-4155-bd87-ae78a1467670",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrapping that in a function\n",
    "\n",
    "def chat(question, history):\n",
    "    result = conversation_chain.invoke({\"question\": question})\n",
    "    return result[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b252d8c1-61a8-406d-b57a-8f708a62b014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7863\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7863/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# And in Gradio:\n",
    "\n",
    "view = gr.ChatInterface(chat, type=\"messages\").launch(inbrowser=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b55e9abb-e1da-46c5-acba-911868aee329",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error in StdOutCallbackHandler.on_chain_start callback: AttributeError(\"'NoneType' object has no attribute 'get'\")\n",
      "Error in StdOutCallbackHandler.on_chain_start callback: AttributeError(\"'NoneType' object has no attribute 'get'\")\n",
      "Error in StdOutCallbackHandler.on_chain_start callback: AttributeError(\"'NoneType' object has no attribute 'get'\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Use the following pieces of context to answer the user's question. \n",
      "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "----------------\n",
      "## Historial de desempeño anual\n",
      "- **2020:**\n",
      "- Completó la incorporación con éxito.\n",
      "- Cumplió con las expectativas en la entrega de los hitos del proyecto. - Recibí comentarios positivos de los líderes del equipo.\n",
      "\n",
      "- **2021:**\n",
      "- Logré una tasa de éxito del 95 % en los plazos de entrega de proyectos.\n",
      "- Recibí el premio \"Rising Star\" en la gala anual de la empresa por contribuciones sobresalientes.\n",
      "\n",
      "- **2022:**\n",
      "- Superé los objetivos al optimizar el código backend existente, mejorando el rendimiento del sistema en un 25 %.\n",
      "- Realicé sesiones de capacitación para desarrolladores junior, fomentando el intercambio de conocimientos.\n",
      "\n",
      "- **2023:**\n",
      "- Lideré una importante revisión de la arquitectura interna de la API, mejorando los protocolos de seguridad.\n",
      "- Contribuí a la transición de la empresa a una infraestructura basada en la nube.\n",
      "- Recibí una calificación de desempeño general de 4.8/5.\n",
      "\n",
      "- **2022**: **Satisfactorio**\n",
      "Avery se centró en reconstruir la dinámica del equipo y abordar las preocupaciones de los empleados, lo que llevó a una mejora general a pesar de un mercado saturado.\n",
      "\n",
      "- **2023**: **Supera las expectativas**\n",
      "Se recuperó el liderazgo del mercado con enfoques innovadores para soluciones de seguros personalizadas. Avery ahora es reconocida en publicaciones de la industria como una voz líder en innovación en tecnología de seguros.\n",
      "\n",
      "## Historial de desempeño anual\n",
      "- **2018**: **3/5** - Jugador de equipo adaptable, pero aún aprendiendo a tomar la iniciativa.\n",
      "- **2019**: **4/5** - Demostró sólidas habilidades para la resolución de problemas, contribución sobresaliente en el proyecto de reclamos.\n",
      "- **2020**: **2/5** - Tuvo dificultades con la gestión del tiempo; se atrasó en los plazos durante un período de lanzamiento de alto tráfico.\n",
      "- **2021**: **4/5** - Logró un cambio significativo con hábitos de trabajo organizados y una gestión de proyectos exitosa.\n",
      "- **2022**: **5/5** - Desempeño excepcional durante la iniciativa \"Innovate\", mostrando liderazgo y creatividad.\n",
      "- **2023**: **3/5** - Mantuvo un trabajo constante; las expectativas de innovación no se cumplieron por completo, lo que llevó a discusiones sobre los objetivos.\n",
      "\n",
      "## Historial de desempeño anual\n",
      "- **2022** - Calificado como \"Supera las expectativas\". Alex Thomson logró el 150 % del objetivo de ventas en los primeros tres meses.\n",
      "- **2023** - Calificado como \"Sobresaliente\". Reconocido por tácticas innovadoras de generación de clientes potenciales que contribuyeron a un aumento del 30 % en los clientes potenciales calificados para el equipo de ventas.\n",
      "\n",
      "### Aspectos destacados:\n",
      "- Mantuvo constantemente un tiempo de respuesta de 30 minutos a los clientes potenciales entrantes.\n",
      "- Coordinó con éxito seminarios web para lanzamientos de productos, que atrajeron a más de 2000 clientes potenciales.\n",
      "Human: ¿Quién recibió el prestigioso premio IIOTY en 2023?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "Respuesta: No sé la respuesta a esa pregunta.\n"
     ]
    }
   ],
   "source": [
    "# Investiguemos qué se envía detrás de escena\n",
    "\n",
    "from langchain_core.callbacks import StdOutCallbackHandler\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.7, model_name=MODEL)\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n",
    "\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory, callbacks=[StdOutCallbackHandler()])\n",
    "\n",
    "query = \"¿Quién recibió el prestigioso premio IIOTY en 2023?\"\n",
    "result = conversation_chain.invoke({\"question\": query})\n",
    "answer = result[\"answer\"]\n",
    "print(\"\\nRespuesta:\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2136153b-d2f6-4c58-a0e3-78c3a932cf55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# crear un nuevo Chat con OpenAI\n",
    "llm = ChatOpenAI(temperature=0.7, model_name=MODEL)\n",
    "\n",
    "# Configurar la memoria de conversación para el chat.\n",
    "memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n",
    "\n",
    "# el recuperador es una abstracción sobre el VectorStore que se utilizará durante RAG; k es la cantidad de fragmentos a utilizar\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 60})\n",
    "\n",
    "# juntando todo: configure la cadena de conversación con GPT 3.5 LLM, el almacén vectorial y la memoria\n",
    "conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5c2bfa3c-810b-441b-90d1-31533f14b1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(question, history):\n",
    "    result = conversation_chain.invoke({\"question\": question})\n",
    "    return result[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c736f33b-941e-4853-8eaf-2003bd988b18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/juangabriel/anaconda3/envs/llms/lib/python3.11/site-packages/gradio/components/chatbot.py:229: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7866\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7866/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "view = gr.ChatInterface(chat).launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "644753e7-17f3-4999-a37a-b6aebf1e4579",
   "metadata": {},
   "source": [
    "# Ejercicios\n",
    "\n",
    "Intenta aplicar esto a tu propia carpeta de datos, de modo que crees un trabajador del conocimiento personal, un experto en su propia información.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b4745a-0a6c-4544-b78b-c827cfec1fb9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
